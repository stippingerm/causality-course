{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14bef34d",
   "metadata": {},
   "source": [
    "# Inference of causal graphs from data\n",
    "\n",
    "Author: Marcell Stippinger\n",
    "\n",
    "Date: 2025-11-07\n",
    "\n",
    "## Contents\n",
    "\n",
    "* Generate logistic map data\n",
    "* Implement time delay embedding\n",
    "* Do convergent cross-mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf234c26",
   "metadata": {},
   "source": [
    "## Imports and plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be97520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats, signal\n",
    "from sklearn.utils import check_random_state\n",
    "from scipy.spatial import cKDTree, distance\n",
    "from typing import NamedTuple, Optional, Tuple, Any\n",
    "# Granger causality test\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7853c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ts(X, E=None, fs=1.0):\n",
    "    \"\"\"Plot time series data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Time series data to plot.\n",
    "    E : array-like, shape (n_samples, n_features), optional\n",
    "        Noise components to overlay on the time series.\n",
    "    fs : float, optional\n",
    "        Sampling frequency of the time series.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    t = np.arange(n_samples) / fs\n",
    "    fig, axes = plt.subplots(n_features, 1, figsize=(6, 4), sharex=True)\n",
    "    if n_features == 1:\n",
    "        axes = [axes]\n",
    "    for i in range(n_features):\n",
    "        axes[i].axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "        axes[i].plot(t, X[:, i])\n",
    "        if E is not None:\n",
    "            axes[i].plot(t, E[:, i], linestyle='None', marker='o', markersize=3, alpha=0.7, label='Noise')\n",
    "        axes[i].set_title(f'Time Series {i+1}')\n",
    "        axes[i].set_ylabel('Value')\n",
    "    axes[-1].set_xlabel('Time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf855ab",
   "metadata": {},
   "source": [
    "## Autocorrelograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d29e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_autocorrelograms(x: np.ndarray, lags: int = 40):\n",
    "    \"\"\"Plot ACF and PACF of a time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like, shape (n_samples,)\n",
    "        Time series data.\n",
    "    lags : int\n",
    "        Number of lags to include in the plots.\n",
    "    \"\"\"\n",
    "    acf_vals = acf(x, nlags=lags)\n",
    "    pacf_vals = pacf(x, nlags=lags)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(6, 4))\n",
    "\n",
    "    axes[0].stem(range(lags + 1), acf_vals)\n",
    "    axes[0].set_title('Autocorrelation Function (ACF)')\n",
    "    axes[0].set_xlabel('Lags')\n",
    "    axes[0].set_ylabel('ACF')\n",
    "\n",
    "    axes[1].stem(range(lags + 1), pacf_vals)\n",
    "    axes[1].set_title('Partial Autocorrelation Function (PACF)')\n",
    "    axes[1].set_xlabel('Lags')\n",
    "    axes[1].set_ylabel('PACF')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add2f92",
   "metadata": {},
   "source": [
    "## Granger causality test\n",
    "\n",
    "We can follow, for example\n",
    "\n",
    "Ding, M., Chen, Y., & Bressler, S. L. (2006). Granger Causality: Basic Theory and Application to Neuroscience. February. https://doi.org/10.1002/9783527609970.ch17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y -> X\n",
    "#gr_yx = grangercausalitytests(coupled_ts, maxlag=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb5f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X -> Y\n",
    "#coupled_ts_rev = np.stack((coupled_ts[:, 1], coupled_ts[:, 0]), axis=1)\n",
    "#gr_xy = grangercausalitytests(coupled_ts[:, ::-1], maxlag=4)\n",
    "#gr_xy = grangercausalitytests(coupled_ts_rev, maxlag=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed46e3",
   "metadata": {},
   "source": [
    "Explain the results\n",
    "\n",
    "- which tests are significant\n",
    "- for what lag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64793d5c",
   "metadata": {},
   "source": [
    "# Coupled logistic maps\n",
    "\n",
    "A single logistic map is generated by\n",
    "$$ x(t+1) = r x(t) (1- x(t)) $$\n",
    "\n",
    "The interaction between logmaps may be additive\n",
    "$$ x(t+1) = r x(t) (1- x(t)) - \\beta y(t) $$\n",
    "or multiplicative\n",
    "$$ x(t+1) = r x(t) (1- x(t) - \\beta y(t)). $$\n",
    "\n",
    "We have to make sure the new value is in $[0, 1]$ therefore we take it modulo $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5bff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _logmap_mul_rhs_modulo(state, r, beta, noise):\n",
    "    \"\"\"\n",
    "    Mapping function of logistic map with circular boundary conditions and\n",
    "    multiplicative coupling\n",
    "\n",
    "    :param state: [X1, X2, ..., Xn], shape (dim, )\n",
    "    :param r: float or array of floats, shape (n, )\n",
    "    :param beta: c_{j-->i} == beta_{ij}, array of floats, shape (n, n)\n",
    "    :param noise: callable, dynamical noise generator\n",
    "    :return: f\n",
    "    \"\"\"\n",
    "    return np.remainder(noise + r * state * (1.0 - state - beta @ state), 1.0)\n",
    "\n",
    "\n",
    "def _logmap_add_rhs_modulo(state, r, beta, noise):\n",
    "    \"\"\"\n",
    "    Mapping function of logistic map with circular boundary conditions and\n",
    "    additive coupling\n",
    "\n",
    "    :param state: [X1, X2, ..., Xn], shape (dim, )\n",
    "    :param r: float or array of floats, shape (n, )\n",
    "    :param beta: c_{j-->i} == beta_{ij}, array of floats, shape (n, n)\n",
    "    :param noise: callable, dynamical noise generator\n",
    "    :return: f\n",
    "    \"\"\"\n",
    "    return np.remainder(noise + r * state * (1.0 - state) - beta @ state, 1.0)\n",
    "\n",
    "def generate_coupled_logmaps(\n",
    "        n_samples: int,\n",
    "        r: Any,\n",
    "        beta: np.ndarray,\n",
    "        noise_std: float = 0.0,\n",
    "        coupling_type: str = 'additive',\n",
    "        random_state: Optional[Any] = None\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate time series from coupled logistic maps with circular boundary conditions.\n",
    "\n",
    "    :param n_samples: Number of time steps to simulate.\n",
    "    :param r: Growth rate parameter(s) for the logistic maps.\n",
    "    :param beta: Coupling matrix, diagonal empty, shape (n, n).\n",
    "    :param noise_std: Standard deviation of the Gaussian noise.\n",
    "    :param coupling_type: Type of coupling ('additive' or 'multiplicative').\n",
    "    :param random_state: Random state for reproducibility.\n",
    "    :return: Time series data, shape (n_samples, n).\n",
    "    \"\"\"\n",
    "    rng = check_random_state(random_state)\n",
    "    n = beta.shape[0]\n",
    "    state = rng.rand(n)\n",
    "    ts = np.zeros((n_samples, n))\n",
    "\n",
    "    if coupling_type == 'additive':\n",
    "        rhs = _logmap_add_rhs_modulo\n",
    "    elif coupling_type == 'multiplicative':\n",
    "        rhs = _logmap_mul_rhs_modulo\n",
    "    else:\n",
    "        raise ValueError(\"coupling_type must be 'additive' or 'multiplicative'\")\n",
    "\n",
    "    for t in range(n_samples):\n",
    "        noise = rng.normal(0, noise_std, size=n)\n",
    "        ts[t] = state\n",
    "        state = rhs(state, r, beta, noise)\n",
    "\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4adda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0 -> x1 avagy x -> y\n",
    "# x0 -> x2 avagy x -> z\n",
    "\n",
    "coupling = np.array([[0.0, 0.2, 0.15],\n",
    "                     [0.0, 0.0, 0.00],\n",
    "                     [0.0, 0.0, 0.00]])\n",
    "\n",
    "ts = generate_coupled_logmaps(\n",
    "    n_samples=1000,\n",
    "    r=3.8,\n",
    "    beta=coupling.T,\n",
    "    coupling_type='additive',\n",
    "    random_state=20251107,\n",
    "    noise_std=0.01\n",
    ")\n",
    "\n",
    "plot_ts(ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7d7c35",
   "metadata": {},
   "source": [
    "# Time delay embedding is based on Takens' theorem.\n",
    "\n",
    "We may use numpy's sliding_window_view to create the time delay embedding. (This is good for us, but not efficient for convolution.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc4ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.lib.stride_tricks.sliding_window_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc1ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_delay_embedding(X: np.ndarray, m: int, tau: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform time-delay embedding of a time series.\n",
    "\n",
    "    :param X: Time series data, shape (n_samples, n_features).\n",
    "    :param m: Embedding dimension.\n",
    "    :param tau: Time delay.\n",
    "    :return: Embedded time series, shape (n_samples - (m - 1) * tau, n_features * m).\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_embedded = n_samples - (m - 1) * tau\n",
    "    embedded = np.zeros((n_embedded, n_features * m))\n",
    "\n",
    "    for i in range(n_embedded):\n",
    "        for j in range(m):\n",
    "            embedded[i, j*n_features:(j+1)*n_features] = X[i + j * tau]\n",
    "\n",
    "    return embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd4d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_delay_embedding_np(X: np.ndarray, m: int, tau: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform time-delay embedding of a time series.\n",
    "\n",
    "    :param X: Time series data, shape (n_samples, n_features).\n",
    "    :param m: Embedding dimension.\n",
    "    :param tau: Time delay.\n",
    "    :return: Embedded time series, shape (n_samples - (m - 1) * tau, n_features * m).\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_embedded = n_samples - (m - 1) * tau\n",
    "    embedded = np.zeros((n_embedded, n_features * m))\n",
    "\n",
    "    embedded = np.lib.stride_tricks.sliding_window_view(X, window_shape=(m, n_features))[:, 0, ::tau, :].reshape(n_embedded, n_features * m)\n",
    "\n",
    "    return embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89114c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyparsing import line\n",
    "\n",
    "\n",
    "def show_embedding_3d(X_embedded: np.ndarray, title: str = ''):\n",
    "    \"\"\"\n",
    "    Visualize 3D time-delay embedding.\n",
    "\n",
    "    :param X_embedded: Embedded time series, shape (n_samples, 3).\n",
    "    :param title: Title of the plot.\n",
    "    \"\"\"\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot(X_embedded[:, 0], X_embedded[:, 1], X_embedded[:, 2], marker='o', linestyle='None', markersize=2, alpha=0.7)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('X(t)')\n",
    "    ax.set_ylabel('X(t + τ)')\n",
    "    ax.set_zlabel('X(t + 2τ)')\n",
    "    plt.show()\n",
    "\n",
    "show_embedding_3d(time_delay_embedding(ts[:, 0:1], m=3, tau=1), title='Time-Delay Embedding of X')\n",
    "show_embedding_3d(time_delay_embedding(ts[:, 1:2], m=3, tau=1), title='Time-Delay Embedding of Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c769f0",
   "metadata": {},
   "source": [
    "# Convergent cross-mapping\n",
    "\n",
    "Map neighborhood of manifold $Y$ (consequence) to indices, look for corresponding values in $x$.\n",
    "Compare the average of these values with the actual $x$.\n",
    "\n",
    "* naive neighbor lookup uses all pairwise distances, $O(n^2)$ complexty\n",
    "* in lower dimensions KDTree algorithm is much more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621cac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(cKDTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a9277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_map(source: np.array, target: np.array, k: int):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        source aka. consequece (np.array): n_samples x d_embed\n",
    "        target aka. cause (np.array): n_samples\n",
    "        k (int): number of neighbors to consider\n",
    "\n",
    "    \"\"\"\n",
    "    tree = cKDTree(source)\n",
    "    # we shall exclude self-matches of points\n",
    "    distances, indices = tree.query(source, k+1)\n",
    "    # search corresponding x values: (n_samples, k)\n",
    "    estimates = target[indices[:, 1:]]\n",
    "    # weights, summing to 1 in every row\n",
    "    weights = np.exp(-distances[:, 1:] / distances[:, 1:2])\n",
    "    weights /= weights.sum(axis=1, keepdims=True)\n",
    "    # estimates\n",
    "    weighted_estimates = (weights * estimates).sum(axis=1)\n",
    "    return weighted_estimates\n",
    "\n",
    "def cross_map_correlation(source: np.array, target: np.array, k: int):\n",
    "    correlations = []\n",
    "    lengths = np.logspace(1, np.log10(len(source)), num=10, dtype=int).astype(int)\n",
    "    for n in lengths:\n",
    "        estimates = cross_map(source[:n], target[:n], k)\n",
    "        corr = np.corrcoef(estimates[:n].flat, target[:n].flat)[0, 1]\n",
    "        correlations.append(corr)\n",
    "    return lengths, correlations\n",
    "\n",
    "def cross_map_effect_lag(source: np.array, target: np.array, k: int, max_lag = 10):\n",
    "    correlations = []\n",
    "    # observed time index of target (cause) relative to source (consequence)\n",
    "    lags = np.arange(-max_lag, max_lag+1) # negative lag takes earlier target (cause) values\n",
    "    T = len(source)\n",
    "    for n in lags:\n",
    "        estimates = cross_map(source[max_lag:T-max_lag], target[max_lag+n:T-max_lag+n], k)\n",
    "        corr = np.corrcoef(estimates.flat, target[max_lag+n:T-max_lag+n].flat)[0, 1]\n",
    "        correlations.append(corr)\n",
    "    return lags, correlations\n",
    "\n",
    "\n",
    "def cross_map_correlation_plot(source: np.array, target: np.array, k: int, title: str = ''):\n",
    "    lengths, correlations = cross_map_correlation(source, target, k)\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.plot(lengths, correlations, marker='o')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    ax.set_xlabel('Library Size')\n",
    "    ax.set_ylabel('Cross Map Correlation')\n",
    "    ax.set_title(title)\n",
    "    ax.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def cross_map_effect_lag_plot(source: np.array, target: np.array, k: int, title: str = ''):\n",
    "    lags, correlations = cross_map_effect_lag(source, target, k)\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.plot(lags, correlations, marker='o')\n",
    "    # integer ticks\n",
    "    ax.get_xaxis().set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    ax.set_xlabel(r'target=cause ahead $\\leftarrow$ lag $\\rightarrow$ source=consequence ahead')\n",
    "    ax.set_ylabel('Cross Map Correlation')\n",
    "    ax.set_title(title)\n",
    "    ax.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a2b425",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "* discuss why target is indexed [:, 0], multiple reasons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee11e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embed = 3\n",
    "\n",
    "x = time_delay_embedding(ts[:, 0:1], m=d_embed, tau=1)\n",
    "y = time_delay_embedding(ts[:, 1:2], m=d_embed, tau=1)\n",
    "\n",
    "print(cross_map_effect_lag(x, y[:, 0], k=6))\n",
    "print(cross_map_effect_lag(y, x[:, 0], k=6))\n",
    "\n",
    "cross_map_effect_lag_plot(x[:], y[:, 0], k=6, title = 'target=Y cross-mapped from source=X (does Y->X?)')\n",
    "cross_map_effect_lag_plot(y[:], x[:, 0], k=6, title = 'target=X cross-mapped from source=Y (does X->Y?)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e65d1",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "* first plot:\n",
    "  * X has an effect on Y that builds up during the time window of length d_embed and helps to somewhat better predict Y\n",
    "* second plot:\n",
    "  * X can be well reconstructed from Y at t=0\n",
    "  * reconstruction of past values decays due to mixing: observed x(t) may come from two different x(t-1) values respectng the map\n",
    "  * future values can be followed and accuracy decays due to chaoticity  \n",
    "\n",
    "TODO:\n",
    "* try different embedding dimensions\n",
    "* how does it influence the height and location of the peak \n",
    "* try lower and higher noise values\n",
    "* how do they influence mixing and chaos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0367ebad",
   "metadata": {},
   "source": [
    "# Cross-mapping coherence\n",
    "\n",
    "Map neighborhood of manifold $Y$ (consequence) to indices, look for corresponding values in $x$.\n",
    "Compare the average of these values with the actual $x$ in spectrum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be443e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_map_coherence(source: np.array, target: np.array, k: int):\n",
    "    n, d = source.shape\n",
    "    estimates = cross_map(source, target, k)\n",
    "    freqs, coherence = signal.coherence(estimates, target, axis=0)\n",
    "    return freqs, coherence\n",
    "\n",
    "def cross_map_coherence_effect_lag(source: np.array, target: np.array, k: int, max_lag = 10):\n",
    "    coherences = []\n",
    "    lags = np.arange(-max_lag, max_lag+1) # same lag convention as in cross_map_effect_lag\n",
    "    T = len(source)\n",
    "    for n in lags:\n",
    "        freqs, coherence = cross_map_coherence(source[max_lag+0:T-max_lag+0], target[max_lag+n:T-max_lag+n], k)\n",
    "        coherences.append(coherence)\n",
    "    return lags, freqs, np.array(coherences)\n",
    "\n",
    "\n",
    "def cross_map_coherence_plot(source: np.array, target: np.array, k: int, title: str = ''):\n",
    "    freqs, coherence = cross_map_coherence(source, target, k)\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.plot(freqs, coherence, marker='o')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    ax.set_xlabel('Frequency')\n",
    "    ax.set_ylabel('Cross Map Coherence')\n",
    "    ax.set_title(title)\n",
    "    ax.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def cross_map_coherence_effect_lag_plot(source: np.array, target: np.array, k: int, title: str = ''):\n",
    "    lags, freqs, coherences = cross_map_coherence_effect_lag(source, target, k)\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    im = ax.matshow(coherences.T, aspect='auto', extent=[lags[0]-0.5, lags[-1]+0.5, freqs[0], freqs[-1]], vmin=0, vmax=1)\n",
    "    cb = plt.colorbar(im, ax=ax)\n",
    "    cb.set_label('Cross Map Coherence')\n",
    "    # integer ticks\n",
    "    ax.get_xaxis().set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    ax.set_xlabel(r'target=cause ahead $\\leftarrow$ lag $\\rightarrow$ source=consequence ahead')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(title)\n",
    "    ax.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d57cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embed = 3\n",
    "\n",
    "x = time_delay_embedding(ts[:, 0:1], m=d_embed, tau=1)\n",
    "y = time_delay_embedding(ts[:, 1:2], m=d_embed, tau=1)\n",
    "\n",
    "cross_map_coherence_plot(x, y[:, 0], k=6, title = 'Y cross-mapped from X (does Y->X?)')\n",
    "cross_map_coherence_plot(y, x[:, 0], k=6, title = 'X cross-mapped from Y (does X->Y?)')\n",
    "\n",
    "cross_map_coherence_effect_lag_plot(x[:], y[:, 0], k=6, title = 'Y cross-mapped from X (does Y->X?)')\n",
    "cross_map_coherence_effect_lag_plot(y[:], x[:, 0], k=6, title = 'X cross-mapped from Y (does X->Y?)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccbaa1c",
   "metadata": {},
   "source": [
    "# Cross-sorting vectors\n",
    "\n",
    "* more computationally heavy as it requires all pairwise distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fdd27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_distance_vector(source: np.array, target: np.array):\n",
    "    n, d = source.shape\n",
    "    sdist = distance.cdist(source, source)\n",
    "    ordering = np.argsort(sdist, axis=0)\n",
    "    tdist = distance.cdist(target, target)\n",
    "    reordered_targets = np.take_along_axis(tdist, ordering, axis=0)\n",
    "    vector = reordered_targets.mean(axis=1)\n",
    "    return vector\n",
    "\n",
    "\n",
    "def cross_distance_vectors_plot(source: np.array, target: np.array, title: str = ''):\n",
    "    vector = cross_distance_vector(source, target)\n",
    "    x = np.arange(len(vector))\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.plot(x, vector, marker='o')\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    ax.set_xlabel('Neighbor index')\n",
    "    ax.set_ylabel('Cross Distance Vector Value')\n",
    "    ax.set_title(title)\n",
    "    ax.grid()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embed = 3\n",
    "\n",
    "x = time_delay_embedding(ts[:, 0:1], m=d_embed, tau=1)\n",
    "y = time_delay_embedding(ts[:, 1:2], m=d_embed, tau=1)\n",
    "\n",
    "print(cross_distance_vector(x, y))\n",
    "print(cross_distance_vector(y, x))\n",
    "\n",
    "cross_distance_vectors_plot(x, y, title = 'Y cross-mapped from X (does Y->X?)')\n",
    "cross_distance_vectors_plot(y, x, title = 'X cross-mapped from Y (does X->Y?)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32c14c",
   "metadata": {},
   "source": [
    "## Recurrence diagrams\n",
    "\n",
    "Based on the paper:\n",
    "* Hirata, Y., & Aihara, K. (2010). Identifying hidden common causes from\n",
    "bivariate time series: A method using recurrence plots.\n",
    "Phys Rev E - Statistical, Nonlinear, and Soft Matter Physics, 81(1), 1–7.\n",
    "https://doi.org/10.1103/PhysRevE.81.016203\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f136cdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Sequence\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import norm\n",
    "from sklearn.utils import check_consistent_length\n",
    "\n",
    "\n",
    "def recurrence_diagrams(\n",
    "        Dx: np.ndarray, Dy: np.ndarray, rate_x: float, rate_y: float\n",
    "):\n",
    "    m = Dx.shape[0]\n",
    "    # number of recurrent elements given the rates\n",
    "    count_x = round(m * rate_x)\n",
    "    count_y = round(m * rate_y)\n",
    "    if (count_x < 1) or (count_y < 1):\n",
    "        raise ValueError(\"invalid threshold, try increasing it\")\n",
    "    # threshold\n",
    "    threshold_x = np.sort(Dx)[count_x]\n",
    "    threshold_y = np.sort(Dy)[count_y]\n",
    "    # recurrence matrices and rejection of independence\n",
    "    Rx = Dx < threshold_x\n",
    "    Ry = Dy < threshold_y\n",
    "    return Rx, Ry\n",
    "\n",
    "\n",
    "def plot_recurrence_diagrams(\n",
    "        X: np.ndarray, Y: np.ndarray, rate_x: float, rate_y: float, ord: float = 2, max_size: int = 100\n",
    "):\n",
    "    Dx = pdist(X, metric='minkowski', p=ord).astype(np.single)\n",
    "    Dy = pdist(Y, metric='minkowski', p=ord).astype(np.single)\n",
    "    Rx, Ry = recurrence_diagrams(Dx, Dy, rate_x, rate_y)\n",
    "    Rx = squareform(Rx)\n",
    "    Ry = squareform(Ry)\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    axes.imshow(Rx[: max_size, :max_size], cmap='Reds', origin='lower', alpha=1.0)\n",
    "    # axes.set_title('Recurrence Diagram X')\n",
    "    axes.imshow(Ry[: max_size, :max_size], cmap='Blues', origin='lower', alpha=0.5)\n",
    "    # axes.set_title('Recurrence Diagram Y')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def independence_test(\n",
    "        Dx: np.ndarray, Dy: np.ndarray, rate_x: float, rate_y: float\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Perform test for H0={x and y are independent}, return the significance\n",
    "    level of unindependence.\n",
    "    The alternative is expressed as the two-sided option of recurrences being\n",
    "    correlated or anticorrelated, although this latter may be impossible. TODO\n",
    "\n",
    "    :param Dx: pairwise distance in X\n",
    "    :param Dy: pairwise distance in Y, same length and same time indices as X\n",
    "    :param rate_x: preferred recurrence rate of X for choosing threshold\n",
    "    :param rate_y: preferred recurrence rate of Y\n",
    "    :return:\n",
    "      * stat: float, almost normally distributed value\n",
    "      * p_stat: float, p-value\n",
    "    \"\"\"\n",
    "    m = Dx.shape[0]\n",
    "    Rx, Ry = recurrence_diagrams(Dx, Dy, rate_x, rate_y)\n",
    "    # number of recurrent elements by chance\n",
    "    mean_j = m * rate_x * rate_y\n",
    "    std_j = np.sqrt(m * (rate_x * rate_y) * (1 - rate_x * rate_y))\n",
    "    _dist = norm(mean_j, std_j)\n",
    "    # rejection of independence\n",
    "    count_j = np.count_nonzero(Rx * Ry)\n",
    "    stat = (count_j - mean_j) / std_j\n",
    "    candidate_p = _dist.cdf(count_j), _dist.sf(count_j)\n",
    "\n",
    "    return stat, 2 * np.minimum(*candidate_p)\n",
    "\n",
    "\n",
    "def directional_coupling_test(\n",
    "        Dx: np.ndarray, Dy: np.ndarray, rate_x: float, rate_y: float\n",
    ") -> Tuple[bool, int, float]:\n",
    "    \"\"\"\n",
    "    Perform test for H0={x drives y}, return the significance\n",
    "    level of no drive from x to y.\n",
    "    The alternative is expressed as the one-sided option of having more\n",
    "    coinciding recurrences in y than expected by chance.\n",
    "\n",
    "    :param Dx: pairwise distance in X\n",
    "    :param Dy: pairwise distance in Y, same length and same time indices as X\n",
    "    :param rate_x: preferred recurrence rate of X for choosing threshold\n",
    "    :param rate_y: preferred recurrence rate of Y\n",
    "    :return:\n",
    "      * cover: bool, whether covering is complete in one direction\n",
    "        (equivalent to the method in the original paper, deprecated)\n",
    "      * achieved_p: float, p-value for maximal achievable covering\n",
    "    \"\"\"\n",
    "    m = Dx.shape[0]\n",
    "    # number of recurrent elements given the rates\n",
    "    Rx, Ry = recurrence_diagrams(Dx, Dy, rate_x, rate_y)\n",
    "    count_y = np.count_nonzero(Ry)\n",
    "    count_j = np.count_nonzero(Rx * Ry)\n",
    "    has_covering = count_j == count_y\n",
    "    # see how far the covering can be pushed with alternative rate_y\n",
    "    mean_c = m * rate_x * rate_y\n",
    "    std_c = np.maximum(\n",
    "        np.sqrt(m * (rate_x * rate_y) * (1 - rate_x * rate_y)),\n",
    "        np.finfo(float).eps)  # make valid dist for rate=0, results in p=0.5\n",
    "    achieved_p = norm.sf(count_j, mean_c, std_c)\n",
    "    return has_covering, achieved_p\n",
    "\n",
    "\n",
    "def _min_sign_rec_rate(m: int, rate: float, alpha: float = 0.01) -> float:\n",
    "    \"\"\"\n",
    "    Minimum significant recurrence rate,\n",
    "    see Eq. 7 in doi:10.1103/PhysRevE.81.016203\n",
    "    \"\"\"\n",
    "    zr = norm.ppf(alpha)  # zr>0 in Eq. 7., but we use zr**2 only\n",
    "    return zr ** 2 * m * rate / (\n",
    "            m ** 2 * (1 - rate) ** 2 + zr ** 2 * m * rate ** 2)\n",
    "\n",
    "\n",
    "def recurrence_analysis(\n",
    "        X: np.ndarray, Y: np.ndarray, ord: float = 2,\n",
    "        rate: float = 0.05, rate_uni: float = 0.5, alpha: float = 0.01\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform all tests used in the decision tree,\n",
    "    see Fig. 2. in doi:10.1103/PhysRevE.81.016203\n",
    "\n",
    "    :param X: embedded time series X\n",
    "    :param Y: embedded time series Y, same length and same time indices as X\n",
    "    :param ord: order of the norm to be used\n",
    "    :param rate: recurrence rate for independence testing\n",
    "    :param rate_uni: recurrence rate for direct cause testing\n",
    "    :param alpha: preferred significance level for direct cause testing\n",
    "    :return:\n",
    "      two arrays, as follows:\n",
    "      * five-case binary output\n",
    "      * three p-values: rejecting independence, rejecting x->y, rejecting y->x\n",
    "    \"\"\"\n",
    "    check_consistent_length(X, Y)\n",
    "    n = len(X)\n",
    "    m = n * (n - 1) // 2\n",
    "    # Note: recurrence matrices require a lot of memory, use single precision.\n",
    "    # Note: pdist calculates half the matrix w/o diagonal and returns a vector.\n",
    "    Dx = pdist(X, metric='minkowski', p=ord).astype(np.single)\n",
    "    Dy = pdist(Y, metric='minkowski', p=ord).astype(np.single)\n",
    "    # If x_|_y (indep) then Rj = Rx * Ry conforms rate_j = rate_x * rate_y.\n",
    "    # H0={independence}, by rejecting H0 we have indication of some causal\n",
    "    # relation, we have to find out which one.\n",
    "    stat, p_independence = independence_test(Dx, Dy, rate, rate)\n",
    "    # If x->y (xtoy) then Rx >= Ry set elements.\n",
    "    # H0={Rx >= Ry possible with p=alpha}, by rejecting H0 we have denied\n",
    "    # direct cause. Note: _min_sign_rec_rate values are very small.\n",
    "    b_xtoy, p_xtoy = directional_coupling_test(\n",
    "        Dx, Dy, rate_uni, _min_sign_rec_rate(m, rate_uni, alpha=alpha))\n",
    "    # Similarly to above.\n",
    "    b_ytox, p_ytox = directional_coupling_test(\n",
    "        Dy, Dx, rate_uni, _min_sign_rec_rate(m, rate_uni, alpha=alpha))\n",
    "    result = np.array([\n",
    "        (p_independence <= alpha) and b_xtoy and not b_ytox,\n",
    "        (p_independence <= alpha) and b_xtoy and b_ytox,\n",
    "        (p_independence <= alpha) and b_ytox and not b_xtoy,\n",
    "        (p_independence <= alpha) and (not b_xtoy) and (not b_ytox),\n",
    "        (p_independence > alpha)\n",
    "    ])\n",
    "    p_values = np.array([p_independence, p_xtoy, p_ytox])\n",
    "    return result, p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6568b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bidir_coupling = np.array([[0.0, 0.2],\n",
    "                           [0.2, 0.0]])\n",
    "\n",
    "bidir_ts = generate_coupled_logmaps(\n",
    "    n_samples=1000,\n",
    "    r=3.8,\n",
    "    beta=bidir_coupling.T,\n",
    "    coupling_type='additive',\n",
    "    random_state=20251107,\n",
    "    noise_std=0.01\n",
    ")\n",
    "\n",
    "\n",
    "d_embed = 3\n",
    "\n",
    "x = time_delay_embedding(ts[:, 0:1], m=d_embed, tau=1)\n",
    "y = time_delay_embedding(ts[:, 1:2], m=d_embed, tau=1)\n",
    "z = time_delay_embedding(ts[:, 2:3], m=d_embed, tau=1)\n",
    "f = time_delay_embedding(bidir_ts[:, 0:1], m=d_embed, tau=1)\n",
    "g = time_delay_embedding(bidir_ts[:, 1:2], m=d_embed, tau=1)\n",
    "\n",
    "\n",
    "plot_recurrence_diagrams(x, y, rate_x=0.05, rate_y=0.05)\n",
    "\n",
    "\n",
    "print('         x->y  x<->y   y->x  x_cc_y indep')\n",
    "print(recurrence_analysis(x, y))\n",
    "print(recurrence_analysis(y, x))\n",
    "print(recurrence_analysis(y, z))\n",
    "print(recurrence_analysis(x, y[np.random.permutation(len(y))]))\n",
    "print(recurrence_analysis(f, g))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a78146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
