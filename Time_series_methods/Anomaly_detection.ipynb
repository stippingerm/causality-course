{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation, global imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy, deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with data containing anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1: Line and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_and_outliers(seed=20241217):\n",
    "    n_total = 70\n",
    "    n_outliers = 30\n",
    "    sigma = 0.04\n",
    "    rs = check_random_state(seed)\n",
    "    x = rs.uniform(-1, 1, n_total)\n",
    "    y = x.copy()\n",
    "    y[n_total-n_outliers:] = rs.uniform(-1, 1, n_outliers)\n",
    "    x += rs.normal(scale=sigma, size=n_total)\n",
    "    y += rs.normal(scale=sigma, size=n_total)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random sample consensus (RANSAC) algorithm\n",
    " \n",
    "* fitting a line to a set of points with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RANSAC:\n",
    "    # Source: https://en.wikipedia.org/wiki/Random_sample_consensus\n",
    "    \n",
    "    def __init__(self, n=10, k=100, t=0.05, d=10, model=None, loss=None, metric=None, random_state=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "\n",
    "        `n`: Minimum number of data points to estimate parameters\n",
    "        `k`: Maximum iterations allowed\n",
    "        `t`: Threshold value to determine if points are fit well\n",
    "        `d`: Number of close data points required to assert model fits well\n",
    "        `model`: class implementing `fit` and `predict`\n",
    "        `loss`: function of `y_true` and `y_pred` that returns a vector\n",
    "        `metric`: function of `y_true` and `y_pred` and returns a float\n",
    "        \"\"\"\n",
    "        self.n = n              # `n`: Minimum number of data points to estimate parameters\n",
    "        self.k = k              # `k`: Maximum iterations allowed\n",
    "        self.t = t              # `t`: Threshold value to determine if points are fit well\n",
    "        self.d = d              # `d`: Number of close data points required to assert model fits well\n",
    "        self.model = model      # `model`: class implementing `fit` and `predict`\n",
    "        self.loss = loss        # `loss`: function of `y_true` and `y_pred` that returns a vector\n",
    "        self.metric = metric    # `metric`: function of `y_true` and `y_pred` and returns a float\n",
    "        self.best_fit = None\n",
    "        self.best_error = np.inf\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        rs = check_random_state(self.random_state)\n",
    "        for _ in range(self.k):\n",
    "            ids = rs.permutation(X.shape[0])\n",
    "\n",
    "            maybe_inliers = ids[: self.n]\n",
    "            maybe_model = copy(self.model).fit(X[maybe_inliers], y[maybe_inliers])\n",
    "\n",
    "            thresholded = (\n",
    "                self.loss(y[ids][self.n :], maybe_model.predict(X[ids][self.n :]))\n",
    "                < self.t\n",
    "            )\n",
    "\n",
    "            inlier_ids = ids[self.n :][np.flatnonzero(thresholded).flatten()]\n",
    "\n",
    "            if inlier_ids.size > self.d:\n",
    "                inlier_points = np.hstack([maybe_inliers, inlier_ids])\n",
    "                better_model = copy(self.model).fit(X[inlier_points], y[inlier_points])\n",
    "\n",
    "                this_error = self.metric(\n",
    "                    y[inlier_points], better_model.predict(X[inlier_points])\n",
    "                )\n",
    "\n",
    "                if this_error < self.best_error:\n",
    "                    self.best_error = this_error\n",
    "                    self.best_fit = better_model\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.best_fit.predict(X)\n",
    "\n",
    "def square_error_loss(y_true, y_pred):\n",
    "    return (y_true - y_pred) ** 2\n",
    "\n",
    "\n",
    "def mean_square_error(y_true, y_pred):\n",
    "    return np.sum(square_error_loss(y_true, y_pred)) / y_true.shape[0]\n",
    "\n",
    "\n",
    "class LinearRegressor:\n",
    "    def __init__(self):\n",
    "        self.params = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        r, _ = X.shape\n",
    "        X = np.hstack([np.ones((r, 1)), X])\n",
    "        self.params = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        r, _ = X.shape\n",
    "        X = np.hstack([np.ones((r, 1)), X])\n",
    "        return X @ self.params\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RANSAC_demo():\n",
    "    regressor = RANSAC(model=LinearRegressor(), loss=square_error_loss, metric=mean_square_error)\n",
    "\n",
    "    x, y = line_and_outliers()\n",
    "    X = x.reshape(-1, 1)\n",
    "\n",
    "    regressor.fit(X, y)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.style.use(\"seaborn-darkgrid\")\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.set_box_aspect(1)\n",
    "\n",
    "    ax.scatter(X, y)\n",
    "\n",
    "    line = np.linspace(-1, 1, num=100).reshape(-1, 1)\n",
    "    ax.plot(line, regressor.predict(line), c=\"peru\")\n",
    "    ax.set_title(\"RANSAC Linear Regression\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "RANSAC_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RANSAC_steps():\n",
    "    k_values = [1, 3, 5, 10]\n",
    "    regressor = RANSAC(model=LinearRegressor(), loss=square_error_loss, metric=mean_square_error, random_state=20241217)\n",
    "\n",
    "    x, y = line_and_outliers()\n",
    "    X = x.reshape(-1, 1)\n",
    "    line = np.linspace(-1, 1, num=100).reshape(-1, 1)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.style.use(\"seaborn-darkgrid\")\n",
    "    fig, axs = plt.subplots(1, len(k_values), figsize=(3*len(k_values), 3), sharex=True, sharey=True)\n",
    "\n",
    "    for k, ax in zip(k_values, axs):\n",
    "        regressor.k = k\n",
    "        regressor.fit(X, y)\n",
    "\n",
    "        ax.scatter(X, y)\n",
    "        ax.set_box_aspect(1)\n",
    "        ax.set_title(f\"k={k}\")\n",
    "\n",
    "        if regressor.best_fit is not None:\n",
    "            ax.plot(line, regressor.predict(line), c=\"peru\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "RANSAC_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection / Novelty detection in point clouds\n",
    "\n",
    "* anomaly: single data series, find the outliers\n",
    "* novelty: training set is normal, find the outliers in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2: two clusters with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_clusters_with_outliers(seed=20241217, n_inliers = 100, n_outliers = 20):\n",
    "    \"\"\"\n",
    "    Generate two clusters of points with outliers.\n",
    "    The clusters are centered at (2, 2) and (-2, -2) with standard deviation 0.3.\n",
    "    The outliers are uniformly distributed in [-4, 4] x [-4, 4].\n",
    "    The inliers come first, followed by the outliers.\n",
    "\n",
    "    returns:\n",
    "    X: np.ndarray, shape=(n_inliers + n_outliers, 2)\n",
    "    ground_truth: np.ndarray, shape=(n_inliers + n_outliers)\n",
    "    \"\"\"\n",
    "    rs = check_random_state(seed)\n",
    "\n",
    "    X_inliers = 0.3 * rs.randn(n_inliers // 2, 2)\n",
    "    X_inliers = np.r_[X_inliers + 2, X_inliers - 2]\n",
    "    X_outliers = rs.uniform(low=-4, high=4, size=(n_outliers, 2))\n",
    "    X = np.r_[X_inliers, X_outliers]\n",
    "\n",
    "    n_outliers = len(X_outliers)\n",
    "    ground_truth = np.ones(len(X), dtype=int)\n",
    "    ground_truth[-n_outliers:] = -1\n",
    "\n",
    "    return X, ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-class Support Vector Machine\n",
    "\n",
    "* based on kernel funcion and SVM\n",
    "* https://scikit-learn.org/dev/modules/sgd.html#online-one-class-svm\n",
    "* https://scikit-learn.org/dev/modules/svm.html#density-estimation-novelty-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_decision_boundary(clf, X_train, X_test, X_outliers):\n",
    "    # Original implementation:\n",
    "    # https://scikit-learn.org/dev/auto_examples/linear_model/plot_sgdocsvm_vs_ocsvm.html\n",
    "\n",
    "    from sklearn.inspection import DecisionBoundaryDisplay\n",
    "    from matplotlib import pyplot as plt\n",
    "    import matplotlib.lines as mlines\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(-4.5, 4.5, 50), np.linspace(-4.5, 4.5, 50))\n",
    "    X = np.concatenate([xx.reshape(-1, 1), yy.reshape(-1, 1)], axis=1)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        X,\n",
    "        response_method=\"decision_function\",\n",
    "        plot_method=\"contourf\",\n",
    "        ax=ax,\n",
    "        cmap=\"PuBu\",\n",
    "    )\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        X,\n",
    "        response_method=\"decision_function\",\n",
    "        plot_method=\"contour\",\n",
    "        ax=ax,\n",
    "        linewidths=2,\n",
    "        colors=\"darkred\",\n",
    "        levels=[0],\n",
    "    )\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        X,\n",
    "        response_method=\"decision_function\",\n",
    "        plot_method=\"contourf\",\n",
    "        ax=ax,\n",
    "        colors=\"palevioletred\",\n",
    "        levels=[0, clf.decision_function(X).max()],\n",
    "    )\n",
    "\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    y_pred_outliers = clf.predict(X_outliers)\n",
    "    n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "    n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "    n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n",
    "\n",
    "    s = 20\n",
    "    b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c=\"white\", s=s, edgecolors=\"k\")\n",
    "    b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c=\"blueviolet\", s=s, edgecolors=\"k\")\n",
    "    c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c=\"gold\", s=s, edgecolors=\"k\")\n",
    "\n",
    "    ax.set(\n",
    "        title=str(clf),\n",
    "        xlim=(-4.5, 4.5),\n",
    "        ylim=(-4.5, 4.5),\n",
    "        xlabel=(\n",
    "            f\"error train: {n_error_train}/{X_train.shape[0]}; \"\n",
    "            f\"errors novel regular: {n_error_test}/{X_test.shape[0]}; \"\n",
    "            f\"errors novel abnormal: {n_error_outliers}/{X_outliers.shape[0]}\"\n",
    "        ),\n",
    "    )\n",
    "    _ = ax.legend(\n",
    "        [mlines.Line2D([], [], color=\"darkred\", label=\"learned frontier\"), b1, b2, c],\n",
    "        [\n",
    "            \"learned frontier\",\n",
    "            \"training observations\",\n",
    "            \"new regular observations\",\n",
    "            \"new abnormal observations\",\n",
    "        ],\n",
    "        loc=\"upper left\",\n",
    "    )\n",
    "\n",
    "\n",
    "def SVM_demo(seed=20241217):\n",
    "    # Original implementation:\n",
    "    # https://scikit-learn.org/dev/auto_examples/linear_model/plot_sgdocsvm_vs_ocsvm.html\n",
    "\n",
    "    from sklearn.kernel_approximation import Nystroem\n",
    "    from sklearn.linear_model import SGDOneClassSVM\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.svm import OneClassSVM\n",
    "\n",
    "    # Generate train data\n",
    "    X = two_clusters_with_outliers(seed=seed, n_inliers=520, n_outliers=20)[0]\n",
    "    X_train = X[10:510]\n",
    "    X_test = np.concatenate((X[:10], X[510:520]), axis=0)\n",
    "    X_outliers = X[520:]\n",
    "\n",
    "    # OCSVM hyperparameters\n",
    "    nu = 0.05\n",
    "    gamma = 2.0\n",
    "\n",
    "    # Fit the One-Class SVM\n",
    "    clf = OneClassSVM(gamma=gamma, kernel=\"rbf\", nu=nu)\n",
    "    clf.fit(X_train)\n",
    "    inspect_decision_boundary(clf, X_train, X_test, X_outliers)\n",
    "\n",
    "    # Fit the One-Class SVM using a kernel approximation and SGD\n",
    "    transform = Nystroem(gamma=gamma, random_state=seed)\n",
    "    clf_sgd = SGDOneClassSVM(\n",
    "        nu=nu, shuffle=True, fit_intercept=True, random_state=seed, tol=1e-4\n",
    "    )\n",
    "    pipe_sgd = make_pipeline(transform, clf_sgd)\n",
    "    pipe_sgd.fit(X_train)\n",
    "    inspect_decision_boundary(pipe_sgd, X_train, X_test, X_outliers)\n",
    "\n",
    "\n",
    "SVM_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest\n",
    "\n",
    "* based on distance from root in a rendom forest classificator\n",
    "* https://scikit-learn.org/dev/modules/outlier_detection.html#isolation-forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsolationForest_demo():\n",
    "    # Original implementation:\n",
    "    # https://scikit-learn.org/dev/auto_examples/ensemble/plot_isolation_forest.html\n",
    "    \n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    X, y = two_clusters_with_outliers()\n",
    "\n",
    "    clf = IsolationForest(max_samples=100, random_state=0)\n",
    "    clf.fit(X)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.inspection import DecisionBoundaryDisplay\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        X,\n",
    "        response_method=\"decision_function\",\n",
    "        alpha=0.5,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    scatter = disp.ax_.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\", cmap='viridis')\n",
    "    handles, labels = scatter.legend_elements()\n",
    "    plt.title(\"Gaussian inliers with \\nuniformly distributed outliers\")\n",
    "    disp.ax_.set_title(\"Path length decision boundary \\nof IsolationForest\")\n",
    "    disp.ax_.axis(\"square\")\n",
    "    disp.ax_.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\n",
    "    plt.colorbar(scatter)\n",
    "    plt.show()\n",
    "    inspect_decision_boundary(clf, X, X[:100], X[100:])\n",
    "\n",
    "\n",
    "IsolationForest_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Outlier Factor (LOF)\n",
    "\n",
    "* based on $k$ nearest neighbors\n",
    "* https://scikit-learn.org/dev/modules/outlier_detection.html#local-outlier-factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOF_demo():\n",
    "    # Original implementation:\n",
    "    # https://scikit-learn.org/dev/auto_examples/neighbors/plot_lof_outlier_detection.html\n",
    "    \n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "    X, ground_truth = two_clusters_with_outliers()\n",
    "    clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "    y_pred = clf.fit_predict(X)\n",
    "    n_errors = (y_pred != ground_truth).sum()\n",
    "    X_scores = clf.negative_outlier_factor_\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.legend_handler import HandlerPathCollection\n",
    "\n",
    "\n",
    "    def update_legend_marker_size(handle, orig):\n",
    "        \"Customize size of the legend marker\"\n",
    "        handle.update_from(orig)\n",
    "        handle.set_sizes([20])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:, 0], X[:, 1], color=\"k\", s=3.0, label=\"Data points\")\n",
    "    # plot circles with radius proportional to the outlier scores\n",
    "    radius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())\n",
    "    scatter = ax.scatter(\n",
    "        X[:, 0], X[:, 1], s=1000 * radius,\n",
    "        edgecolors=\"r\", facecolors=\"none\", label=\"Outlier scores\",\n",
    "    )\n",
    "    ax.set_xlabel(\"prediction errors: %d\" % (n_errors))\n",
    "    ax.legend(\n",
    "        handler_map={scatter: HandlerPathCollection(update_func=update_legend_marker_size)}\n",
    "    )\n",
    "    ax.set_title(\"Local Outlier Factor (LOF)\")\n",
    "    plt.show()\n",
    "\n",
    "LOF_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection in time series\n",
    "\n",
    "* Takens' theorem, 1981\n",
    "* time-delay embedding: interpret values in a sliding window as elements of a vector\n",
    "* use a metric to find non-typical configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series discord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine_with_baseline_shift(seed=20241217):\n",
    "    shift = 2.5\n",
    "    noise = 0.05\n",
    "    rs = check_random_state(seed)\n",
    "    t = np.linspace(0, 20 * np.pi, num=1000)\n",
    "    y = np.sin(t)\n",
    "    y[600:] += shift\n",
    "    y[500:600] = np.linspace(0, shift, num=100)\n",
    "    y += noise * rs.randn(1000)\n",
    "    return t, y\n",
    "\n",
    "\n",
    "def plot_sine_with_baseline_shift(mark=(), windows=(), title=None):\n",
    "    from matplotlib import pyplot as plt\n",
    "    t, y = sine_with_baseline_shift()\n",
    "    if title is None:\n",
    "        title = \"Sine wave with baseline shift\"\n",
    "    plt.plot(t, y)\n",
    "    mark = list(mark)\n",
    "    plt.scatter(t[mark], y[mark], c=\"red\")\n",
    "    for window in windows:\n",
    "        plt.axvspan(t[window[0]], t[window[1]], color=\"gray\", alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_discord(ts: np.ndarray, window_size: int, n_discords: int):\n",
    "    \"\"\"\n",
    "    Find the `n_discords` most dissimilar subsequences in `ts` using a sliding window of size `window_size`.\n",
    "\n",
    "    Parameters:\n",
    "    `ts`: np.ndarray, shape=(n_samples,)\n",
    "    `window_size`: int\n",
    "    `n_discords`: int\n",
    "    \"\"\"\n",
    "    from scipy.spatial.distance import cdist\n",
    "    from scipy.linalg import toeplitz\n",
    "\n",
    "    n_samples = len(ts)\n",
    "    n_windows = n_samples - window_size + 1\n",
    "    windows = np.lib.stride_tricks.sliding_window_view(ts, window_shape=(window_size,))\n",
    "\n",
    "    # Calculate pairwise Euclidean distances between windows\n",
    "    distances = cdist(windows, windows, metric=\"euclidean\")\n",
    "    # Make overlapping windows have infinite distance\n",
    "    distances += toeplitz([np.inf] * window_size + list(np.zeros(n_windows - window_size)))\n",
    "    # Note: it n_samples is large, this is very inefficient and memory-intensive\n",
    "    # a more efficient way is to use cKDTree and discard overlapping vectors\n",
    "\n",
    "    discord_ids = np.argsort(distances.min(axis=1))[-n_discords:]\n",
    "    return discord_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_time_series_discord():\n",
    "    window_size = 50\n",
    "    t, y = sine_with_baseline_shift()\n",
    "    discord_ids = time_series_discord(y, window_size=window_size, n_discords=10)\n",
    "    print(discord_ids)\n",
    "    plot_sine_with_baseline_shift(windows=[(discord_ids[0], discord_ids[0] + window_size)],\n",
    "                                  title=\"Time series discord detection\")\n",
    "\n",
    "demo_time_series_discord()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal outlier factor\n",
    "\n",
    "* anomalous events occured rarely, similar points are concentrated in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_outlier_factor(ts: np.ndarray, window_size: int, n_outliers: int):\n",
    "    \"\"\"\n",
    "    Find the `n_outliers` most concentrated subsequences in `ts` using a sliding window of size `window_size`.\n",
    "\n",
    "    Parameters:\n",
    "    `ts`: np.ndarray, shape=(n_samples,)\n",
    "    `window_size`: int\n",
    "    `n_discords`: int\n",
    "    \"\"\"\n",
    "    from scipy.spatial import cKDTree\n",
    "\n",
    "    num = 20\n",
    "    n_samples = len(ts)\n",
    "    n_windows = n_samples - window_size + 1\n",
    "    windows = np.lib.stride_tricks.sliding_window_view(ts, window_shape=(window_size,))\n",
    "\n",
    "    # Calculate how far similar sequences in time\n",
    "    _, indices = cKDTree(windows).query(windows, k=num + 1)\n",
    "    temporal_outlier_factor = np.mean(np.abs(indices[:, 1:] - np.arange(n_windows)[:, None]), axis=1)\n",
    "    # Generate reference values because the achievable distance depends on the index\n",
    "    # The paper gives an exact formula for this, std. dev can be calculated too\n",
    "    equispaced = np.linspace(0, n_windows, num=num)\n",
    "    reference = np.mean(np.abs(equispaced - np.arange(n_windows)[:, None]), axis=1)\n",
    "\n",
    "    outlier_ids = np.argsort(temporal_outlier_factor / reference)[:n_outliers]\n",
    "    return outlier_ids\n",
    "\n",
    "def demo_temporal_outlier_factor():\n",
    "    window_size = 50\n",
    "    t, y = sine_with_baseline_shift()\n",
    "    outlier_ids = temporal_outlier_factor(y, window_size=window_size, n_outliers=10)\n",
    "    print(outlier_ids)\n",
    "    plot_sine_with_baseline_shift(mark=outlier_ids,\n",
    "                                  title=\"Temporal outlier factor detection\")\n",
    "\n",
    "demo_temporal_outlier_factor()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection with deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 3: pump sensor data\n",
    "\n",
    "* Download from https://www.kaggle.com/datasets/nphantawee/pump-sensor-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data'\n",
    "ext = '.csv.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"nphantawee/pump-sensor-data\")\n",
    "ext = '.zip'\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "\n",
    "df = pd.read_csv(f\"{path}/sensor{ext}\", index_col=0)\n",
    "df['machine_status'] = (df['machine_status'] == 'NORMAL').astype(int)\n",
    "df = df.dropna(how='all', axis=1).dropna(how='any', axis=0)\n",
    "print(df.shape, df['machine_status'].sum())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(df[df['machine_status'] == 1].drop(columns=['timestamp', 'machine_status']))\n",
    "scaled = ss.transform(df.drop(columns=['timestamp', 'machine_status']).fillna(0))\n",
    "print(scaled.shape)\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(scaled[df['machine_status'] == 1], shuffle=False)\n",
    "train_dataset = np.lib.stride_tricks.sliding_window_view(train_dataset,64,axis=0)\n",
    "val_dataset = np.lib.stride_tricks.sliding_window_view(val_dataset,64,axis=0)\n",
    "whole_dataset = np.lib.stride_tricks.sliding_window_view(scaled,64,axis=0)\n",
    "print(train_dataset.shape, val_dataset.shape, whole_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement pytorch autoencoder for sequence length n_seq and number of features n_features\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_features, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, hidden_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, n_features),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, criterion, optimizer):\n",
    "    train_dataset = DataLoader(train_dataset, batch_size=512, shuffle=False)\n",
    "    val_dataset = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    best_loss = np.inf\n",
    "    history = dict(train=[], val=[])\n",
    "\n",
    "    for epoch in tqdm(range(10)):\n",
    "        model = model.train()\n",
    "\n",
    "        train_losses = []\n",
    "        for seq_true in train_dataset:\n",
    "            seq_true = torch.flatten(seq_true, 1).to(device)\n",
    "            seq_pred = model(seq_true)\n",
    "            loss = criterion(seq_pred, seq_true)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        val_losses = []\n",
    "        model = model.eval()\n",
    "        with torch.no_grad():\n",
    "            for seq_true in val_dataset:\n",
    "\n",
    "                seq_true = torch.flatten(seq_true, 1).to(device)\n",
    "                seq_pred = model(seq_true)\n",
    "\n",
    "                loss = criterion(seq_pred, seq_true)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "\n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = deepcopy(model.state_dict())\n",
    "    return model, history, best_model_wts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(n_features=np.prod(train_dataset.shape[1:]), hidden_dim=8)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.L1Loss(reduction='sum').to(device)\n",
    "\n",
    "\n",
    "trained, history, best_model_wts = train_model(\n",
    "    model.to(device),\n",
    "    torch.tensor(train_dataset, dtype=torch.float32),\n",
    "    torch.tensor(val_dataset, dtype=torch.float32),\n",
    "    criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_anomalies(model, test_dataset, criterion):\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    model = model.eval()\n",
    "    test_dataset = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for seq_true in test_dataset:\n",
    "            seq_true = torch.flatten(seq_true, 1).to(device)\n",
    "            seq_pred = model(seq_true)\n",
    "            loss = criterion(seq_pred, seq_true).mean(axis=1)\n",
    "            losses.append(loss.cpu().numpy())\n",
    "    return np.concatenate(losses, axis=0)\n",
    "\n",
    "losses = find_anomalies(\n",
    "    model,\n",
    "    torch.tensor(whole_dataset, dtype=torch.float32),\n",
    "    criterion = nn.L1Loss(reduction='none').to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses():\n",
    "    from matplotlib import pyplot as plt\n",
    "    fig, ax = plt.subplots()\n",
    "    at = ax.twinx()\n",
    "    at.plot(losses)\n",
    "    at.set_ylabel('Loss')\n",
    "    ax.plot(df['machine_status'].values, color='red')\n",
    "    ax.set_ylabel('Machine status')\n",
    "    ax.set_xlabel('Time')\n",
    "    plt.show()\n",
    "\n",
    "plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read more...\n",
    "\n",
    "* More implementations (RNN, LSTM): https://medium.com/@artur.shaikhatarov/anomaly-detection-using-recurrent-neural-networks-autoencoders-41bdf52d7b53\n",
    "* Other datasets\n",
    "  * https://developer.ibm.com/tutorials/iot-deep-learning-anomaly-detection-5/\n",
    "  * https://github.com/claimed-framework/component-library/blob/master/component-library/anomaly/anomaly-score-unsupervised/test-anomaly-score-unsupervised.ipynb\n",
    "  * https://compete.hexagon-ml.com/practice/competition/39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
